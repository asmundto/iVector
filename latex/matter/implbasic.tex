\chapter{Data Preparation}

\section{Training and Development Corpus}

To train the systems we used data from the CallFriend corpus. This database contains several hours of unscripted telephone conversations for twelve languages. The languages are shown in table \ref{tab:callfriend}.
\begin{table}[hbt]
	\begin{center}
	\caption{Languages in the CallFriend corpus}
	\begin{tabular}{ | c | c | c | c |}
	\hline
	American English & Canadian French & Egyptian Arabic & Farsi \\ \hline
	German & Hindi & Japanese & Korean \\ \hline
	Mandarin Chinese & Spanish & Tamil & Vietnamese \\ \hline
	\hline
	\end{tabular}
	\label{tab:callfriend}
	\end{center}
\end{table}
For the English, Mandarin and Spanish languages the database includes speech from two dialects.The dialects are given in table \ref{tab:callfrienddial}.
\begin{table}[hbt]
	\begin{center}
	\caption{Dialects included in the CallFriend Database}
	\begin{tabular}{| l | c | c |}
		\cline{2-3}
		\multicolumn{1}{l}{}  & \multicolumn{2}{|c|}{\textbf{Dialects}} \\ \hline
		\textbf{American English} & Non-Southern & Southern \\ \hline 
		\textbf{Mandarin Chinese} & Mainland & Taiwan \\ \hline 
		\textbf{Spanish} & Non-Caribbean & Caribbean \\
		\hline
	\end{tabular}
	\label{tab:callfrienddial}
	\end{center}
\end{table}	
For each language or dialect there is about 60 hours of speech. Approximately 50 minutes of speech from each dialect was used as development data. This data set is used to test the performance of the system during development. By using a separate data set for the final evaluation, the performance of the system will be given from unseen data. If implementation decisions were to be influenced by the data set used for the final performance evaluation, then the resulting score might be too optimistic. To make sure that the system recognizes languages and not persons, no speaker was present in both the training and development set. We aimed for having few speakers in the development set in order for the systems to be trained on as many speakers as possible.

\section{Evaluation Set}

For the final evaluation of the systems the 2003 NIST Language Recognition Evaluation set was used \footnote{\url{http://www.itl.nist.gov/iad/mig/tests/lre/2003/}}. By using a standardized test set, we can easily compare the performance of our system with others. The NIST set mostly includes data collected for (but not used in) the CallFriend corpus \cite{martin2003nist} so the conditions for the test segments should be similar to the training data. The NIST set includes the same twelve target languages that were included in the training set given in table \ref{tab:callfriend}. The set also includes out of set utterances in Russian. The Russian segments can be used to test the systems ability to recognize if any of the trained languages were spoken at all. In accordance with the evaluation rules \cite{martin2003nist}, no attempts were made to prepare the system specifically for Russian out of set segments.

The NIST set includes data of 3, 10 and 30 seconds duration of speech. While our main focus has been on the 30 second segments, we will report results for the other segment lengths as well. The segment lengths were enforced by using an automatic speech activity algorithm to split utterances to the correct size. For each language and duration at least 80 segments from 40 speakers were provided. 

\section{Phoneme Transcription}
\label{sect:phonetranscription}

Phoneme transcription for all the systems were performed using the Brno University of Technology (BUT) phoneme recognizer \footnote{\url{http://speech.fit.vutbr.cz/software/phoneme-recognizer-based-long-temporal-context}}. The recognizers utilizes the long temporal context features as explained in section \ref{sect:phnrec}. The phoneme recognizer was trained on Hungarian utterances and has previously performed well on language recognition tasks \cite{lrivector, torres2008mitll}. The phoneme recognizer is trained distinguish $62$ classes. The high number of classes can make training material sparse when using higher order $n$-grams. In order to reduce the sparsity any token labeled as noise were ignored, and consecutive tokens of silence were stripped to just one token. Experiments were also performed using a many-to-one token mapping suggested in \cite{torres2008mitll}. With this mapping, information about phoneme duration is ignored resulting in only $32$ different tokens. By itself, we expect this loss of information to negatively impact the performance of the system. However, this mapping can severly reduce the training time for some of the systems. This could potentially mean that we could train the systems with more data, or use higher-order $n$-grams.

After utterances have been transcribed, we split the transcription into parts so that e.g. the length of a development set utterance match the length of the test set utterances. Since the system's performance should be dependent on the utterance length, this should make development scores more closely approximate the score we would expect during final evaluations. 


\section{Gaussian Backend}
\label{sect:implgaussback}

For processing of the score-vectors we used the Focal Multiclass Toolkit \footnote{\url{https://sites.google.com/site/nikobrummer/focalmulticlass}}. The toolkit can be applied to a wide range of machine-learning problems, but it includes methods that are specifically designed for the NIST language recognition evaluation.  E.g. find detection threshold values that minimizes the expected cost, $C_{\text{det}}$, of a recognition decision. It also implements training and evaluation using GMMs which were discussed in section \ref{sect:backendscoring}. An issue with the backend was that it couldn't be trained on score-vectors from the training set. This is because the language models and classifiers are expected to perform very well  when tested on it's training data. The score-vectors would then be unrepresentative for the score-vectors we would expect from unseen data. A possible solution would be to introduce a separate training set just for the backend training. Instead of reducing the ammount of training material for other system components, we opted for training the backend with the development data. Since we don't have a set for development testing of the backend, we now couldn't experiment with- and compare different backends. For this reason we used simple and easily trained models in the backend that should ensure reasonable results. 

For each language, a single Gaussian Multivariate Model was trained. We followed the recommandation given in \cite[p. 70]{matejkalre} to treat dialects as separate languages in the rest of the system, and fuse the scores for these languages in the backend. This makes the single Gaussian Model an obvious source of bias error, since we expect that score-vectors from utterances of different dialects will be located in separate clusters of score-vectors. On the other hand, we could easily end up with an over-fitted model if we were to represent languages with Gaussian Mixtures. 

\subsection{Universal Background Model for Out-of-Set Languages}

The Gaussian backend was also responsible for recognizing the out-of-set language in the evaluation set. It would be possible to train a language model to distinguish most other languages from the target languages. This approach was taken in \ref[p. 55]{matejkalre} with good results, but it would require training material for an ensemble of out-of-set languages. For our systems, we make the assumption that the score vector from an out-of-set language will be contained in the same region in the score-space as utterances from all the target languages. We train an Universal Background Model (UBM) using all the development data from target languages. The UBM is just another Gaussian Mixture that can then synthesize utterances for any language. Due to the risks of over-fitting, a single Gaussian component was used for the out-of-set language as well. Since the single Gaussian component has to model score-vectors from more than one language, we expect this component to have a higher variance than the components for single languages. This means that the model will give a target language higher likelihood than the UBM if an unknown utterance is scored close to the mean of a "single-language" component. Thus the UBM creates the desired effect of requireing some confidence before we assign an utterance to any target language at all. Still, this simplification is expected to degrade the performance of the system.

\section{Baseline System}

The baseline system is an implementation of the smoothed language model described in chapter \ref{sect:baselinetheory}. For each language, one model is created for up to phoneme trigrams. The $k$-parameter that determines the model's degree of smoothing was set independently for each language. A range of parameter values were tested for each language, and the parameter that maximized the log-likelihood in equation \ref{basescore} was chosen. Since smoothing adds bias to the model, the likelihood for the training set will decrease for a higher value of $k$. Because of this, the total likelihood over the development set was used for the tests. An other strategy might have been to use the same value for $k$ for each language. We could then select the value that maximized the identification rate using equation \ref{lreiddec} with the likelihoods produced from the model. The former approach was used since trigrams not seen in the training set appeared with clearly different frequency for each language. This indicated that each model would require a different degree of smoothing. It did not seem feasible to maximize the identification rate using different $k$-values for each language as the parameter search-space would be too great. 

The identification rate for 30, 10 and 3 second development utterances all seemed to benefit slightly from using the unmapped phoneme transcript and it was therefore used for the final system. The log-likelihoods produced by the language model was somewhat unsuited to use as score-vector for the Gaussian backend. This is because the likelihoods are affected by the number of  phonemes in the utterance. Instead we calculated the posterior probability for each class using equal priors for the score-vector. This can be viewed as a normalization of the score-vector.