\chapter{Identification Results}

For presenting identification results, it seemed beneficial to not use the likelihoods from the Gaussian back-end, and simply select the language that was given the highest likelihood by the language model or classifier. This enables us to compare the results from the NIST 2003 set with the development set.

\section{Baseline system}

Table \ref{baseidresults} shows the identification performance of the baseline system on 30 second utterances from both the NIST and development set. For many of the languages there is a big difference in performance for the two sets. This variation seems to be because we used few different speakers in the dev set. For Vietnamese utterances where the error-rate is $0$\% and $27.7$\% on the NIST and development set respectively, all the errorunous classifications were from utterances by one speaker. For this speaker only $11$ out of $39$ utterances were correctly identified. This just illustrates why it was important to train the system on as many speakers as possible, the distribution of phonemes will vary amongst speakers, so it should be beneficial to train the system on a diverse set of speakers. The average identification rate over languages were $93.3$\% and $91.1$\% for the NIST-set and development set respectively. $93.0$\% of the 30 second NIST-utterances were correctly identified, against $92.1$\% of the development utterances. 
\begin{table}[hbt!]
\begin{tabular}{| l | c | c | c | c | c | c | c | c | c | c | c | c |}
\hline
Language &Ar & En & Fa & Fr & Ge & Hi & Ja & Ko & Ma & Sp & Ta & Vi \\
\hline
Arabic & 72 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
English & 2 & 228 & 0 & 4 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
Farsi & 2 & 1 & 72 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
French & 1 & 0 & 0 & 71 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
German & 0 & 0 & 0 & 1 & 75 & 0 & 2 & 0 & 0 & 0 & 0 & 0 \\
Hindi & 0 & 0 & 0 & 0 & 0 & 72 & 0 & 0 & 0 & 1 & 0 & 0 \\
Japanese & 0 & 0 & 0 & 1 & 4 & 0 & 137 & 1 & 0 & 1 & 0 & 0 \\
Korean & 0 & 2 & 2 & 1 & 0 & 4 & 8 & 79 & 2 & 0 & 0 & 0 \\
Mandarin & 1 & 4 & 1 & 1 & 0 & 3 & 4 &  0 & 75 & 0 & 0 & 0 \\
Spanish & 1 & 1 & 0 & 0 &  0 & 0 & 4 & 0 & 0 & 76 & 0 & 0 \\
Tamil & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 79 & 0 \\
Vietnamese & 0 & 3 & 4 & 1 & 0 & 0 & 4 & 0 & 2 & 0 & 1  & 80 \\
\hline
NIST ER \% & 10.0 & 5.0 & 10.0 & 11.3 & 6.3 & 10.0 & 14.4 & 1.3 & 6.3 & 5.0 & 1.3 & 0.0 \\
\hline
Dev ER \% & 1.9 & 2.4 & 9.9 & 7.9 & 23.8 & 9.0 & 3.0 & 0.0 & 2.0 & 5.4 & 13.9 & 27.7 \\
\hline
\end{tabular}
\caption{Confusion matrix from identifying 30 second documents from the 30 second NIST 2003 evaluation set with the baseline system. The columns denote the true identity of the document, while rows denote the identified language. ER is the error-rate of the system, which is reported for both the development set and NIST set in the last two rows.}
\label{baseidresults}
\end{table}

\section{iVector system}

In table \ref{ivectidresults} we show the identification rate of the iVector system. Compared to the baseline system, we got a much higher identification rate on Vietnamese and Tamilian development utterances. German development utterances were however a problem for both systems. MORE

\begin{table}[hbt!]
\begin{tabular}{| l | c | c | c | c | c | c | c | c | c | c | c | c |}
\hline
Language &Ar & En & Fa & Fr & Ge & Hi & Ja & Ko & Ma & Sp & Ta & Vi \\
\hline
Arabic & 100 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
English & 0 & 197  & 1 & 0 & 17 & 4 & 0 & 0 & 2 & 0 & 0 & 0 \\
Farsi & 1  & 0 & 91 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
French & 1  & 0 & 1 & 93 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
German & 0  & 0 & 0 & 0 & 77 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
Hindi & 0  & 0 & 4 & 2 & 0 & 91 & 0 & 0 & 0 & 0 & 7 & 3 \\
Japanese & 0 & 0 & 0 & 1 & 2 & 2 & 97 & 0 & 0 & 4 & 3 & 2 \\
Korean & 0  & 4 & 0 & 0 & 4 & 3 & 2 & 101 & 2 & 1 & 2 & 15 \\
Mandarin & 0  & 0 & 3 & 0 & 0 & 0 & 1 & 0 & 198 & 0 & 0 &  8 \\
Spanish & 0  & 0 & 0 & 3 & 0 & 0 & 0 & 0 & 0 & 191 & 2 & 0 \\
Tamil & 0  & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 87 & 0 \\
Vietnamese & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 3 &  0 & 73 \\
\hline
Error-rate \%& 1.9 & 2.4 & 9.9 & 7.9 & 23.8 & 9.0 & 3.0 & 0.0 & 2.0 & 5.4 & 13.9 &  27.7 \\
\hline
\end{tabular}
\caption{Confusion matrix from identifying 30 second development documents with the baseline system. The columns denote the true identity of the document, while rows denote the identified language.}
\label{ivectidresults}
\end{table}

\section{Discussion}



\chapter{Detection Results}

