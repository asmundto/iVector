\chapter{Conclusion}

For this thesis we constructed an iVector language recognition system reaching a similar performance as the baseline PRLM system. Resetting iVectors to zero during training of the total variability matrix seems to offer an interesting alternative to the standard training method to prevent over-fitting of the model, but it would not be surprising if the learning method is too naive for data that is harder to over-fit. It is not known if the reset trained system also was over-fitted or perhaps too biased to perform optimally. 


This suggests that there might have been a total variability matrix more suited for language recognition



%If we were to perform multiple updates of the iVectors in each iteration, the reset training should resemble the standard training more closely. 


% This begs to question if less sparse document vectors were needed to fully exploit the advantages of the iVector subspace modelling.


%If we actually had to settle for an biased learning method




%to fully exploit the advantages of iVector subspace modelling

