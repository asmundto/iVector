\chapter{Theory}
\label{sect:Theory}

\section{Introduction to languages}

\section{Language Recognition systems}

\subsection{Language Detection vs Identification}
\label{sect:detvsid}

Identification and detection of languages are two very related tasks in language recognition. In both tasks languages are constricted to be in a set of classes. Each class can be a single language, dialect, or a set of languages, and the goal of our system is to separate these classes. In this thesis we will only look at formulations where each class is a single language and possibly one class that consists of all other languages, which we refer to as an \emph{out of set language}. We call it an \emph{open set} recognition problem When the set includes an out of set language, and \emph{closed set} otherwise. With this formulation in mind, we drop the notion of classes, and just call each class a language. 

Given a hypothesized language, a language detection system will either accept or reject the claimed language based on a set of observations. The confidence the language detection system requires for its decisions will vary on application, but we say that we accept the hypothesis if the probability that it is the hypothesized language, $l_i$, given the systems knowledge of languages, $\theta$, and observations, $S$, is greater than some threshold, $t$. The acceptance criteria can also be written as

\begin{equation*}
P(l_i | \theta, X) \geq t.
\end{equation}
Any claim that doesn't satisfy the equation will be rejected.

 In language identification, our goal is just to find the most probable language from the known languages, that is
\begin{equation*}
\underset{i}{\arg \max} P(l_i | \theta, X).
\end{equation}
Since both recognition problems are quite straight forward after we've found $P(l_i | \theta, X)$, most effort in the field of language recognition is put into making $\theta$ a good model of the languages of interest.

\section{Language Model Using Markov Chains}














\section{More on the iterative update process}

Here we are going to look more closely at the Newton Raphson update steps for producing i-vectors and the variability matrix. In a real-time implementation we would need to extract i-vectors using these updates for live data which makes the operation time-sensitive. At the same time these updates involve linear algebra on a high-dimension space, making the operations computationally expensive. An unefficient implementation of these updates will therefore severely impact the computational requirements for training and live usage of the system. It is also critical for the total performance of the system that the i-vectors convey meaningful information about an utterance. A thorough study of the Newton Raphson update process is therefore warranted. 

\subsection{Solving the Newton Raphson systems}

To avoid issues with numeric instability, it is often desirable not to calculate the inverse of a matrix \cite[p. 743]{cormen}. We can rewrite the linear systems in equation ??? and ??? to

\begin{equation}\label{lupeqN}
\mathbf{H}_n(\text{old}) \delta\mathbf{w}_n = \mathbf{g}_n
\end{equation}
and
\begin{equation}\label{lupeqC}
\mathbf{H}_c(\text{old}) \delta \mathbf{t}_c = \mathbf{g}_c(\text{old})
\end{equation}
respectively where $\delta$ means the difference between the new and old vectors. It is beneficial to ensure that these equations have one, and just one, solution. More than one solution would indicate that some of the dimensions in the rows of $\mathbf{T}$ or i-vector is redundant, making us solve a more complicated problem than strictly needed. The requirements on $\mathbf{T}$ and i-vectors to guarantee just one solution, is shown in appendix \ref{posdefproof}. As long as our goal is to find global relationships between utterances, and not overfit i-vectors to each utterance (by letting the i-vector dimension approach the number of training utterances or unique features), these requirements should be met. One exception is when we update rows of $\mathbf{T}$ that correspond to features not seen in the training set. Since it is unlikely that we gain much information from such features anyways, we can assume that those rows are always all zero. The rows can then be ignored during i-vector updates without much, if any, loss in performance.

In appendix \ref{posdefproof} we also show that the Hessian in equation \ref{lupeqN} and \ref{lupeqC} are positive definite. This enables us to use simple algorithms like LU decomposition to solve the systems \cite[p. 749]{cormen}. With an i-vector dimension of $R$, LU decomposition will solve the systems in $\mathcal{O}(R^3)$ asymptotic time CORMEN REF. While there are faster solvers for positive definite systems like the $\mathcal{O}(R^2)$ solver in \cite{tewfik1994fast}, $R$ should be of a size that probably doesn't necessitate excessive optimization.

In equation ??? $\mathbf{H}_n$ is found by calculating the outer product of rows of $\mathbf{T}$ $C$ times, making the asymptotic runtime $\Omega(CR^2)$. Similary for $\mathbf{H}_c$ the asymptotic runtime of equation ??? is  $\Omega(NR^2)$. Since $R$ should be significantly less than $N$ and $C$ to ensure that the linear systems only have one solution, calculating the Hessian will be more computationally demanding than solving the resulting linear systems. In appendix \ref{symproof} we show that the Hessian is symmetric, which enables us to only calculate the upper (or lower) half of the Hessians. While the asymptotic runtime remains the same, the actual runtime of should be nearly halved. 

\subsection{Achieving higher performance}

In an iterative algorithm it is important to ensure that each iteration brings you closer to the solution of the problem. The Newton Raphson method is oblivious to high-order derivatives, and we only use an approximation to the Hessian, so an increase in likelihood is not guaranteed from updates using equation \ref{lupeqN} and \ref{lupeqC}. In \cite{kockmann2010prosodic}, Kockmann et. al. would halve the update step until the likelihood from equation ??? increased when updating either row of $\mathbf{T}$ or i-vectors. If a higher likelihood wasn't achieved after some attempts, the old vector would be used.  

A common problem in svm classification is that the feature vector is dominated by some high-variance feature \cite{wan2005speaker}. In our subspace modeling, a high-variance in the counts of some trigrams might be a cause of concern as well. A good model of such features would likely be crucial to maximize equation ???. Not only because the number of occurrences will vary greatly between utterances, but also since a high variance implies that the feature occurs very frequently in at least some utterances. Many dimensions of the i-vector may then be spent on accurately controlling the multinomial distributions probability of producing those features. It is not clear if we gain much information from this precise fitting, rather than having coarser knowledge about the exact frequency of those features, and having more degrees of freedom in the i-vector to model other features. In \cite{soufifar2011ivector} the i-vector system's performance increased when the square root of elements in the document vector, $\gamma_n$, was used instead. By taking the square root, frequent features will be a smaller part of the total feature count, and the variance will be decreased. This should spread the importance of modeling each feature more evenly across the whole document vector.



In order to achieve a high likelihood from equation ???, $\mathbf{T}$ needs to model the variability between utterances. T

We will here show that the achievable likelihood only depends on the column span of $\mathbf{T}$. This means that techniques like orthogonalizing $\mathbf{T}$ won't have any effect on 




\section{Gaussian backend}


