
\label{sect:Theory}

\chapter{Introduction to languages}

\chapter{Language Recognition systems}

\section{Language Detection vs Identification}
\label{sect:detvsid}

Identification and detection of languages are two very related tasks in language recognition. In both tasks languages are constricted to be in a set of classes. Each class can be a single language, dialect, or a set of languages, and the goal of our system is to separate these classes. In this thesis we will only look at formulations where each class is a single language and possibly one class that consists of all other languages, which we refer to as an \emph{out of set language}. We call it an \emph{open set} recognition problem When the set includes an out of set language, and \emph{closed set} otherwise. With this formulation in mind, we drop the notion of classes, and just call each class a language. 

Given a hypothesized language, a language detection system will either accept or reject the claimed language based on a set of observations. The confidence the language detection system requires for its decisions will vary on application, but we say that we accept the hypothesis if the probability that it is the hypothesized language, $l_i$, given the systems knowledge of languages, $\theta$, and observations, $S$, is greater than some threshold, $t$. The acceptance criteria can also be written as

\begin{equation*}
p(l_i | \theta, X) \geq t.
\end{equation*}
Any claim that doesn't satisfy the equation will be rejected.

 In language identification, our goal is just to find the most probable language from the known languages, that is
\begin{equation*}
\underset{i}{\arg \max} p(l_i | \theta, X).
\end{equation*}
This means that unless we are implementing a system for a real application, the distinction between the two language recognition problems aren't that important so long as we find $P(l_i | \theta, X)$. What is more important is that the model of the languages, $\theta$, is suited to distinguish languages.

\section{System Overview}
\label{sect:sysoverview}

It could be possible to write a set of rules that scores the utterances against languages. The approach taken by many recent language recognition systems is to build a statistical model from training data. Without speculating on how much time it would take to create a rule based system, a statistical driven system should at least be easier to extend to new languages.


\section{Phoneme recognizer}
\label{sect:phnrec}




\section{Language models}
\label{sect:basiclangmodel}

The responsibility of the language model is to output a single score per language from a phoneme sequence $S$. Since our final recognition decision will be based on the probability that an utterance belongs to a language, $p(l_i, | S)$, it seems natural to have the scores approximate that probability. This is a somewhat strict requirement for systems as described in section \ref{sect:sysoverview} since the scores will be further processed by the backend. At the very least, a good language model should give scores that are easily transformable to an estimate of this probability. 

In order for the system to perform well for languages with limited available training data, it will be beneficial that the language model is easily trained. A too complex model will not be able to see patterns in the data, it will instead be over-fitted to explain each utterance independently which makes it unfit to score utterances outside of the training set \cite[311]{information}. Over-fitting can be reduced by using a larger training set \cite[147]{machinelearningbook}, but it is clearly an undesirable feature for a language models. This is also the reason why we need a separate data set to evaluate the recognition system. The systems recognition performance on the data it was trained for will not necessarily extend to unseen data.

If we introduce the notion that training and test data are generated from a random process, the systems ability to correctly estimate $p(l_i | S)$, and thereby make correct recognition decisions, will be given by two error sources called model \emph{bias} and \emph{variance} \cite[149]{machinelearningbook}. \emph{Bias} is high if the system consistently over- or underestimate $p(l_i | S)$ for some documents regardless of the training set. In this way it represents the systems inability to correctly predict certain documents. Model \emph{variance} is the variance in the probability estimate of an utterance belonging to a language when the model is trained with different data. It represents the systems sensitivity to noise in the training data. Over-fitting is a symptom of a high variance system where the model is only expected to give correct probability estimates for utterances that are very close to a training utterance. A system with limited training data can generally not have both low variance and bias \cite[312]{information}, so a concession between the two errors has to be made.

This was only a shallow introduction to language models due to the variety of techniques used in language recognition systems. A more detailed survey will be given when we discuss specific language models in section \ref{sect:basetrain} and ???.




Since $p(S)$ is independent of the spoken language, it only serves as a constant to enforce that the probability of the utterance belonging to any language is $1$. The prior distribution of languages, $p(l_i)$, will depend on the application and should be left to the decision unit to adjust to it's needs. A suited score that can be used to make theoretically optimal decisions based on the phone sequence will then be $p(S|l_i)$.This is a somewhat strict requirement since the socres will be further processed by the Gaussian backend, investigated in section ??, before any decisions are made. . A model that returns $p(S|l_i)$ will be a theoretically optimal score for the utterance with no loss of information other than what was incurred during tokenization.




\section{Back-end Calibration of Score Vectors}
\label{sect:backendscoring}

The back-end will calibrate the scores from the language model into the posterior, $p(l_i | S, \theta)$, so that so that they can be applied to the recognition decisions discussed in section \ref{sect:detvsid}. Like the language model, this calibration can be trained using real data. A separate calibration step allows us to ignore application dependent information like what other languages should the system recognize, and the prior probability of a user speaking each language. The language models for each language can then be trained independently since it only needs to return some number that in some way correlates with the posterior probability for it's language. Even if the language model is trained to return estimates of the posterior, there may still be some benefits calibrating the score vectors as there might exist patterns in the scores that can be exploited. Furthermore, the back-end can be used to fuse scores for an utterance from multiple systems using different phoneme recognizers or language models. The fused system is then expected to perform better than each of the individual systems as long as the errors each system makes are somewhat uncorrelated \cite[818]{lidbok}.

\subsection{Gaussian Backend}

It is likely that the best calibration method will depend on the nature of the score-vectors given from the language models. A very flexible tool to perform calibration that impose little demands on the score vector is to use a multivariate Gaussian mixture model (GMM). With this framework, the score vector, $\mathbf{y}$, for a given language is assumed to be produced from a generative statistical model. The likelihood of a $r$-dimensional Gaussian component $k$ to produce a score vector $\mathbf{y}$ is then

\begin{equation}
\label{gmmeq}
p(\mathbf{y} | \mathbf{\mu}_k, \mathbf{\Sigma}_k, K = k) = \frac{1}{(2\pi)^{r/2}|\mathbf{\Sigma}_k|^{1/2}}\exp(-\frac{1}{2}(\mathbf{y}-\mathbf{\mu_k})^T\mathbf{\Sigma}_k^{-1}(\mathbf{x}-\mathbf{\mu_k}))
\end{equation}
where $\mathbf{\mu_k}$ is the $r$-dimensional mean of the score vectors produced by the mixture, $\mathbf{\Sigma}_k$ is the covariance of the mixture and $|\mathbf{\Sigma}_k|$ it's determinant \cite[p. 94]{talegk}. From Bayes rule we have that
\begin{align}
p(K=k | \mathbf{\mu}_k, \mathbf{\Sigma}_k, \mathbf{y}) &= 
\frac{p(\mathbf{y} | \mu_k, \Sigma_k, K= k) \cdot p(K=k )}{p(\mathbf{y})} \nonumber \\
&= \frac{p(\mathbf{y} | \mu_k, \sigma_k, K=k) \cdot p(K=k)}{\sum_{i = 1}^{|K|} p(\mathbf{y} | \mu_i, \Sigma_i, K=i) \cdot p(K=i)} \label{gmmchoose}
\end{align}
where $p(K=k)$ is the prior probability of the score vector being generated from mixture $k$. If we know what mixture each score vector in a training set belongs to, then $\mathbf{\mu}_k$ can be estimated as
\begin{equation}
\label{muest}
\mathbf{\mu}_k = \operatorname{E}(\mathbf{y} | \mathbf{y} \in k)
\end{equation}
and $\mathbf{\Sigma}_k$ as
\begin{equation}
\label{sigmaest}
\mathbf{\Sigma}_k = \operatorname{E}\left( (\mathbf{y}-\mathbf{\mu}_k)(\mathbf{y}-\mu_k)^T | \mathbf{y} \in k \right)
\end{equation}
where $E(a|b)$ is the conditional expectation \cite[p. 94]{talegk}. The power of the Gaussian mixture model we have now described is that it is enable to model any probability distribution \cite[p. 95]{talegk}. Even for a training set, we will generally only know the language of an utterance, but not the mixture the score vector belongs to. This means that equation \ref{muest} and \ref{sigmaest} cannot directly be solved. We can still estimate the parameters using the iterative EM-algorithm covered in section \ref{sect:emgmmest}. With one mixture per language, equation \ref{muest} and \ref{sigmaest} can be solved by noting that $\mathbf{y} \in k$ is equivalent to that the score vector stems from the given language, $l_i$.

Desisjon/threshold

\subsection{Gaussian Mixture Parameter Estimation Using Expectation Maximization}
\label{sect:emgmmest}

In the EM-algorithm, the number of mixtures per language is assumed to be fixed to some value. Initial values for $\mu$, $\Sigma$ and priors for mixtures are random or from some crude estimate. Since we know the language of all score-vectors used for training, we will estimate parameters for the given language using score-vectors from only that language. The presented algorithm is taken from \cite[o. 439]{machinelearningbook}

For each step we first estimate the probability that the score vector is generated from each of the mixtures using equation \ref{gmmchoose}. When finding new estimates for $\mu_k$ and $\Sigma_k$ we can weight the contribution of a score-vector to a cluster with the probability that it was generated from that mixture. The new value for $\mu_k$, called $\mu_k(\text{NEW})$ is then

\begin{equation}
\label{emmuest}
\mu_k(\text{NEW}) = \frac{1}{\hat{N}_k}\sum_{\forall \mathbf{y}} p(K=k | \mu_k, \Sigma_k, \mathbf{y})\mathbf{y}
\end{equation}
and $\Sigma_k(\text{NEW})$ is given by

\begin{equation}
\label{emsigmaest}
\mathbf{\Sigma}_k(\text{NEW}) = \frac{1}{\hat{N}_k}\sum_{\forall \mathbf{y}} p(K=k | \mu_k, \Sigma_k, \mathbf{y})(\mathbf{y}-\mu_k(\text{NEW}))(\mathbf{y}-\mu_k(\text{NEW})^T
\end{equation}
where $\hat{N}_k$ is 
\begin{equation}
\label{emnkest}
\hat{N}_k = \sum_{\forall \mathbf{y}} p(K=k | \mu_k, \Sigma_k, \mathbf{y}).
\end{equation}
We can then update the priors for each cluster using the new means and variances,
\begin{equation}
\label{empriorest}
p(K=k) = \frac{\sum_{\forall \mathbf{y}}p(K=k | \mu_k(\text{NEW}), \Sigma_k(\text{NEW}), \mathbf{y})}{\sum_{\forall \mathbf{y}} 1}.
\end{equation}
These steps are repeated until some convergence criterion is met.

It turns out that the best fitted model to any training data would be to have as many mixtures as training vectors, each with a covariance of 0. This is clear since $\mathbf{y}$ is continuous, so making the GMM generate only a discrete set of vectors is the only way to get a non-zero probability of generating exactly $\mathbf{y}$. Such a model would clearly be over-fitted to the training data. In order to avoid a model with too high variance, the number of mixtures per language must be carefully constrained.

\subsection{Recognition Decisions and System Performance}


\chapter{Language Modeling Using Markov Chains}

We will here look at how the baseline system utilizes the phoneme sequences from the phone recognizer to construct language models, which then are used to differentiate languages with other phoneme sequences. This is the only module that the baseline system exclusively uses. Intuitively the best performing system will be determined by the total information loss from its modules, so the performance difference between the two systems will be determined by the information loss from assumptions made in this module versus the iVector specific modules. In section \ref{sect:basetrain} we will present how to train a model and use it to decode an utterance. In section \ref{sect:basesmooth} the concept of smoothing the language models will be presented to make the language model better fit unseen utterances.

\section{Scoring Utterances}
\label{sect:basescore}

\section{Model training}
\label{sect:basetrain}

Using Bayes rule, the probability that the utterance stems from language $l_i$, will be

\begin{equation}
\label{basebayes}
p(l_i|S) = \frac{p(S|l_i) \cdot p(l_i)}{p(S)}.
\end{equation}


An estimate of this score could be found by letting $p(S|l_i, \theta)$ equal the frequency the phoneme sequence appears in the training data for that language. Unfortunately the number of possible phoneme sequences grows exponentially with the sequence length, $N$. With an almost infinite number of parameters to train for the model, it will be impossible to train. 

The probability of the phoneme sequence can also be written as chain of random events
\begin{equation}
\label{chainproduct}
p(S|l_i) = \prod_{j=1}^Np(s_i|S_1^{j-1}, l_i)
\end{equation}
where $S_a^b = s_{\max(a,1)}, s_{\max(a+1,1)}, ... , s_{\max(b, 1)}$. We can limit the number of parameters that need to be estimated by making the $(n-1)$-th order Markov assumption \cite[p. 376]{talegk}, that the outcome of the event occurring at time $i$, only depends on the $n-1$ previous outcomes. We call the phoneme sequence $S_{j-n+1}^j$ an $n$-gram. Equation \ref{chainproduct} can then be approximated to
\begin{equation}
\label{markovchain}
p(S|l_i) \approx \prod_{j=1}^Np(s_j|S_{j-n+1}^{j-1},l_i).
\end{equation}
Our model can then be estimated from training data as
\begin{equation}
\label{baselinetrain}
p(s_j|S_{j-n+1}^j, l_i, \theta) = 
\begin{cases} 
\dfrac{C_i(S_{j-n+1}^j)}{C_i(S_{j-n+1}^{j-1})} & \text{if $n \geq 2$} \\ \\
\dfrac{C_i(s_j)}{C_i(s)} & \text{if $n = 1$}
\end{cases}
\end{equation}
where $C_i(S_a^b)$ is the number of times the phoneme sequence $S_a^b$ occurs- and $C_i(s)$ the total number of phonemes in the training data for language $l_i$. Using these estimates, an unknown utterance can be scored for each language by

\begin{equation}
\label{basescore}
p(S|l_i, \theta) = \prod_{j=1}^Np(s_j|S_{j-n+1}^{j-1}, l_i, \theta).
\end{equation}
Since the first $n$ phonemes in equation \ref{basescore} has less than $n-1$ preceding phonemes, the transition probabilities given less than $n$ previous states has to be estimated as well.

We can view $p(s_j|S_{j-n+1}^j, l_i)$ as multinomial distributions (one for each state, $S_{j-n+1}^j$). With $C_i(S_{j-n+1}^{j-1})$ draws from the distribution the variance in $p(s_j|S_{j-n+1}^j, l_i, \theta)$ will be
\begin{equation}
\label{baselinevar}
\operatorname{VAR}(p(s_j|S_{j-n+1}^j, l_i, \theta)) = \frac{p(s_j|S_{j-n+1}^{j-1}, l_i)(1-p(s_j|S_{j-n+1}^{j-1}, l_i))}{C_i(S_{j-n+1}^{j-1})}
\end{equation}
 By definition $C_i(S_{j-n+1}) \leq C_i(S_{j-(n-1)+1})$ so using a smaller value for $n$ will produce a model with less variance. But this will also make the model more biased, as it will only capture short term dependencies between phonemes. Returning to the discussion in section \ref{sect:basiclangmodel}, $n$ has to be set as a tradeoff between model bias and variance. Clearly a too high value for $n$ will make the model unfit to measure the probability of unseen sequences.

\section{Model Smoothing}
\label{sect:basesmooth}

A problem with equation \ref{basescore} is that any trigram not observed in the training set will make the probability of observing the whole sequence zero.  Furthermore, the granularity of equation \ref{baselinetrain} is no more than $C_i(S_{j-n+1}^{j-1})$. This means that for rare events, the relative error between the true and estimated probabilities is unbounded. By estimating the probability of rare events with lower order $n$-grams, the granularity will increase.With only a few observation of an $n$-gram, it is unlikely that we will find useful dependencies between phonemes spaced far apart, making a high order Markov assumption unnecessary complicated. The method described is known as backoff smoothing \cite[p. 559]{talegk}. The smoothed probability, $\hat{p}$ of a $n$-gram will be given by

\begin{equation}
\label{basesmooth}
\hat{p}(s_j|S_{j-n+1}^{j-1}, l_i, \theta) = 
\begin{cases}
p(s_i|S_{j-n+1}^{j-1}, l_i, \theta) & \text{if $C_i(S_{j-n+1}^j > k_i)$} \\
\alpha_i(S_{j-n+1}^{j-1})\hat{p}(s_j|S_{j-n+2}^{j-1}, l_i, \theta) & \text{if $C_i(S_{j-n+1}^j \leq k_i$)}
\end{cases}
\end{equation}
where $k_i$ is some possibly language dependent constant, and $\alpha_i(S_{j-n+1}^{j-1})$ is a constant that makes the sum of probabilities for any $n$-gram with a given history equal $1$. We find $\alpha_i$ by 
\begin{align}
\sum_{s_j:C_i(S_{j-n+1}^j \leq k} p(s_j | S_{j-n+1}^{j-1}, l_i, \theta)
&= \sum_{s_j:C_i(S_{j-n+1}^j \leq k} \alpha_i(S_{j-n+1}^{j-1})\hat{p}p(s_j | S_{j-n+2}^{j-1}, l_i, \theta) \nonumber \\
\alpha_i(S_{j-n+1}^{j-1})
&= \frac{\sum_{s_j:C_i(S_{j-n+1}^j \leq k}p(s_i|S_{j-n+1}^{j-1}, l_i, \theta)}{\sum_{s_j:C_i(S_{j-n+1}^j \leq k}\hat{p}(s_j | S_{j-n+2}^{j-1}, l_i, \theta)} \nonumber \\
&=\frac{1-\sum_{s_j:C_i(S_{j-n+1}^j > k}p(s_i|S_{j-n+1}^{j-1}, l_i, \theta)}{1-\sum_{s_j:C_i(S_{j-n+1}^j > k}p(s_j | S_{j-n+2}^{j-1}, l_i, \theta)}
\end{align}
Since both the granularity and variance should be good for unigrams ($1$-grams), the values for $\alpha_i$ can be found iteratively for larger values of $n$ by assuming that the unigram frequency requires no smoothing. When much of the probability mass for a given context belongs to frequently seen events, $\alpha_i$ will likely penalize having to use lower order $n$-gram statistics which is a useful trait. 

\chapter{More on the iterative update process}

Here we are going to look more closely at the Newton Raphson update steps for producing i-vectors and the variability matrix. In a real-time implementation we would need to extract i-vectors using these updates for live data which makes the operation time-sensitive. At the same time these updates involve linear algebra on a high-dimension space, making the operations computationally expensive. An unefficient implementation of these updates will therefore severely impact the computational requirements for training and live usage of the system. It is also critical for the total performance of the system that the i-vectors convey meaningful information about an utterance. A thorough study of the Newton Raphson update process is therefore warranted. 

\section{Solving the Newton Raphson systems}

To avoid issues with numeric instability, it is often desirable not to calculate the inverse of a matrix \cite[p. 743]{cormen}. We can rewrite the linear systems in equation ??? and ??? to

\begin{equation}\label{lupeqN}
\mathbf{H}_n(\text{old}) \delta\mathbf{w}_n = \mathbf{g}_n
\end{equation}
and
\begin{equation}\label{lupeqC}
\mathbf{H}_c(\text{old}) \delta \mathbf{t}_c = \mathbf{g}_c(\text{old})
\end{equation}
respectively where $\delta$ means the difference between the new and old vectors. It is beneficial to ensure that these equations have one, and just one, solution. More than one solution would indicate that some of the dimensions in the rows of $\mathbf{T}$ or i-vector is redundant, making us solve a more complicated problem than strictly needed. The requirements on $\mathbf{T}$ and i-vectors to guarantee just one solution, is shown in appendix \ref{posdefproof}. As long as our goal is to find global relationships between utterances, and not overfit i-vectors to each utterance (by letting the i-vector dimension approach the number of training utterances or unique features), these requirements should be met. One exception is when we update rows of $\mathbf{T}$ that correspond to features not seen in the training set. Since it is unlikely that we gain much information from such features anyways, we can assume that those rows are always all zero. The rows can then be ignored during i-vector updates without much, if any, loss in performance.

In appendix \ref{posdefproof} we also show that the Hessian in equation \ref{lupeqN} and \ref{lupeqC} are positive definite. This enables us to use simple algorithms like LU decomposition to solve the systems \cite[p. 749]{cormen}. With an i-vector dimension of $R$, LU decomposition will solve the systems in $\mathcal{O}(R^3)$ asymptotic time CORMEN REF. While there are faster solvers for positive definite systems like the $\mathcal{O}(R^2)$ solver in \cite{tewfik1994fast}, $R$ should be of a size that probably doesn't necessitate excessive optimization.

In equation ??? $\mathbf{H}_n$ is found by calculating the outer product of rows of $\mathbf{T}$ $C$ times, making the asymptotic runtime $\Omega(CR^2)$. Similary for $\mathbf{H}_c$ the asymptotic runtime of equation ??? is  $\Omega(NR^2)$. Since $R$ should be significantly less than $N$ and $C$ to ensure that the linear systems only have one solution, calculating the Hessian will be more computationally demanding than solving the resulting linear systems. In appendix \ref{symproof} we show that the Hessian is symmetric, which enables us to only calculate the upper (or lower) half of the Hessians. While the asymptotic runtime remains the same, the actual runtime of should be nearly halved. 

\subsection{Achieving higher performance}

In an iterative algorithm it is important to ensure that each iteration brings you closer to the solution of the problem. The Newton Raphson method is oblivious to high-order derivatives, and we only use an approximation to the Hessian, so an increase in likelihood is not guaranteed from updates using equation \ref{lupeqN} and \ref{lupeqC}. In \cite{kockmann2010prosodic}, Kockmann et. al. would halve the update step until the likelihood from equation ??? increased when updating either row of $\mathbf{T}$ or i-vectors. If a higher likelihood wasn't achieved after some attempts, the old vector would be used.  

A common problem in svm classification is that the feature vector is dominated by some high-variance feature \cite{wan2005speaker}. In our subspace modeling, a high-variance in the counts of some trigrams might be a cause of concern as well. A good model of such features would likely be crucial to maximize equation ???. Not only because the number of occurrences will vary greatly between utterances, but also since a high variance implies that the feature occurs very frequently in at least some utterances. Many dimensions of the i-vector may then be spent on accurately controlling the multinomial distributions probability of producing those features. It is not clear if we gain much information from this precise fitting, rather than having coarser knowledge about the exact frequency of those features, and having more degrees of freedom in the i-vector to model other features. In \cite{soufifar2011ivector} the i-vector system's performance increased when the square root of elements in the document vector, $\gamma_n$, was used instead. By taking the square root, frequent features will be a smaller part of the total feature count, and the variance will be decreased. This should spread the importance of modeling each feature more evenly across the whole document vector.



In order to achieve a high likelihood from equation ???, $\mathbf{T}$ needs to model the variability between utterances. T

We will here show that the achievable likelihood only depends on the column span of $\mathbf{T}$. This means that techniques like orthogonalizing $\mathbf{T}$ won't have any effect on 




