\chapter{Introduciton}

The purpose of automatic language recognition is to identify which language is spoken from a speech sample. Applications of language recognition are e.g. adding metadata to audio/video databases or  a preprocessing step for multilingual spoken user interfaces. One of the most popular approaches to language recognition is based on phonotactics, i.e. the assumption that the distribution of sound combinations, phone n-grams, is a distinguishing trait of a language. In these systems, a phone recognizer first creates a phonemic representation of the speech. The number of occurrences of phone n-grams in the recognized phone sequence is stored in a spoken document vector. 

In the traditional approach, the document vector is scored against statistical phone n-gram models for each language to determine which language is the most likely. An alternative approach vector space modeling where the document vector is transformed from a pure count to a representation which emphasizes sound combinations distinctive to a language and de-emphasizes combinations which occur uniformly across languages. 

Finally, it is possible to use the document vector directly for classification, using discriminative classifiers to separate the languages. This is the topic of this thesis. One problem with direct classification is that the dimensionality of the document vectors is very large. If the number of phones in the phone recognizer is 50, and we count 3-grams, the dimension is 125 000. By nature, the vectors are sparse, since only a fraction of the 125 000 possible 3-grams will occur in e.g. a 30-second utterance. Training classifiers on full-sized document vectors will be computationally wasteful, in addition to being difficult due to the limited coverage represented by any realistic training set. Techniques to reduce the vector's dimension is thus usually employed. iVector subspace modeling is a recent approach to dimensionality reduction that has shown promising results. The compressed vectors, called iVectors, are found through maximum likelihood estimation, making it significantly different from minimum squared error techniques like latent semantic mapping or principal component analysis. The assignment is to examine the iVector approach, construct an iVector language recognition system, and evaluate its performance against more traditional language recognizers.