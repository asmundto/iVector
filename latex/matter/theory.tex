\chapter{Theory}
\label{sect:Theory}

\section{Properties of the Newton Raphson updates}

Here we are going to look more closely at the Newton Raphson update steps for producing iVectors and the variability matrix. In a real-time implementation we would need to extract iVectors using these updates for live data which makes the operation time-sensitive. At the same time these updates involve linear algebra on a high-dimension space, making the operations computationally expensive. An unefficient implementation of these updates will therefore severely impact the training and testing time of the system.
Due to the similarities in updating rows of $\mathbf{T}$ and iVectors we will here use a common notation when proofs apply to both updates. $\mathbf{x}$ will denote the updated vector (either a row of $\mathbf{T}$ or an iVector), $\mathbf{H}_x$ its jacobian and $\mathbf{g}_x$ its gradient. We will use $\mathbf{y}$ to denote the vectors used to update $\mathbf{x}$ (iVectors if $\mathbf{x}$ is a row of $\mathbf{T}$ and vice versa).

Due to numeric instability, it is often undesirable to calculate the inverse of a matrix when solving a set of linear equations CORMEN743. This can be avoided by rewriting equation ??? and ??? to

\begin{equation}\label{lupeq}
\mathbf{H}\delta\mathbf{x} = \mathbf{g}
\end{equation}
where $\delta\mathbf{x}$ equals $\mathbf{x}(NEW)-\mathbf{x}(OLD)$. Equation \ref{lupeq} can then be solved by using well known algorithms such as LUP decomposition. With a given iVector dimension $R$, the system is then solved in $\mathcal{O}(R^3)$ asymptotic time CORMEN754. 

LUP decomposition can only solve the system when $\mathbf{H}$ is singular. That is when the rank of $\mathbf{H}$ is $R$, and there exists one and only one solution ELEMLINALGEBRA. This is not the case when we during training update a row of $T$ corresponding to an unseen feature, $c$. From equation ??? and ??? both $\mathbf{H}_c$ and $\mathbf{g}_c$ will then be all zero \footnote{If we estimate $\mathbf{m}$ with the same data that we use to update $\mathbf{T}$, then equation ??? and ??? gives that $\mathbf{m}_c$ will equal $-\infty$, making $\phi_{nc}=\gamma_{nc}=0$ for all utterances, $n$, in the training set.}. Since it is clear that any values for $\mathbf{t}_c$ will be a valid solution, a simple way to circumvent the problem is to assume that the new values for $\mathbf{t}_c$ is also zero. This will make the gradient and jacobian for iVectors (equation ??? and ???) unaffected by features not seen in the training data. Since it is unlikely that we gain much information  from those features anyways, we will have reduced the complexity of updating iVectors and $\mathbf{T}$ by ignoring these rows, without significant- or perhaps any cost.

There are other situations where $\mathbf{H}$ can be singular, but they shouldn't arise MORE!!!


From equations ??? and ???, $\mathbf{H}_x$ can be seen to be symmetric.
\begin{equation}\label{symproof}
\mathbf{H}_x^T = \left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)^T = 
\sum_{\forall y}a_{xy}\left(\mathbf{y}\mathbf{y}^T\right)^T =
 \sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T = \mathbf{H}_x
\end{equation}
where $a_{xy}$ is the scalar part of the jacobians equation. This is a very important property as the construction of the jacobian is computationally expensive. Naively, the outer vector product used in the jacobian requires $R^2$ multiplications and is repeated $NC$ times when updating all $N$ iVectors or $C$ rows of $\mathbf{T}$, making the asymptotic runtime $\mathcal{O}(NCR^2)$.  Since most of the matrix is redundant, we only have to calculate the upper or lower half of the matrix. While the asymptotic runtime remains the same, the actual running time of the algorithm should be nearly halved.

Under normal circumstances, the jacobian is also postive definite. A matrix, $\mathbf{A}$ is given to be positive definite if $\mathbf{z}^T\mathbf{Az} > 0$ for all non-zero vectors $\mathbf{z}$ CORMEN. 

\begin{align}
\mathbf{z}^T\mathbf{H}_x\mathbf{z} &= \mathbf{z}^T\left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)\mathbf{z} 
= \sum_{\forall y}a_{xy}\mathbf{z}^T\mathbf{y}\mathbf{y}^T\mathbf{z} 
= \sum_{\forall y}a_{xy}\left(\mathbf{z}^T\mathbf{y}\right)^2 \geq 0 
\end{align}
The inequality above is given by the fact that $a_{xy} \geq 0$ and that $\mathbf{z}^T\mathbf{y}$ is a real scalar. Equality can only be reached if $\mathbf{z}$ is orthogonal to all vectors $\mathbf{y}$ where $a_{xy}$ is non-zero. 

Equality can only be reached if there is at least one index $i$ of the vectors $\mathbf{y}$ where $y_i$ is always zero when $a_xy$ is nonzero. If this were to happen then the jacobian would also be singular which as previously stated should be avoidable. 

Given that $R$ should be significantly less than $N$ and $C$, setting up the jacobian will be more expensive than solving the system. Given that $R$ should be significantly less than $N$ and $C$, setting up the jacobian will be more expensive than solving the system.

