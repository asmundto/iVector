\chapter{Theory}
\label{sect:Theory}

\section{Properties of the Newton Raphson updates}

Here we are going to look more closely at the Newton Raphson update steps for producing iVectors and the variability matrix. In a real-time implementation we would need to extract iVectors using these updates for live data which makes the operation time-sensitive. At the same time these updates involve linear algebra on a high-dimension space, making the operations computationally expensive. An unefficient implementation of these updates will therefore severely impact the computational requirements for training and live usage of the system. It is also critical for the total performance of the system that the iVectors convey meaningful information about an utterance. A thorough study of the Newton Raphson update process is therefore warranted. 

Due to the similarities in updating rows of $\mathbf{T}$ and iVectors we will here use a common notation when proofs apply to both updates. $\mathbf{x}$ will denote the updated vector (either a row of $\mathbf{T}$ or an iVector), $\mathbf{H}_x$ its (approximate) Hessian and $\mathbf{g}_x$ its gradient. We will use $\mathbf{y}$ to denote the vectors used to update $\mathbf{x}$ (iVectors if $\mathbf{x}$ is a row of $\mathbf{T}$ and vice versa).

\subsection{Solving linear systems of equations}

To avoid issues with numeric instability, it is often desirable not to calculate the inverse of a matrix \cite[p. 743]{cormen}. We can rewrite the linear systems in equation ??? and ??? to

\begin{equation}\label{lupeq}
\mathbf{H}_x\delta\mathbf{x} = \mathbf{g}
\end{equation}
where $\delta\mathbf{x}$ equals $\mathbf{x}(NEW)-\mathbf{x}(OLD)$. Equation \ref{lupeq} can then be solved by using well known algorithms such as LUP decomposition. With a given iVector dimension $R$, the system is then solved in $\mathcal{O}(R^3)$ asymptotic time \cite[p. 754]{cormen}. 

LUP decomposition can only solve this system when $\mathbf{H}_x$ is nonsingular. That is when the rank of $\mathbf{H}_x$ is $R$, and there exists one and only one solution \cite[p. 54]{matte3}. This is not the case when we during training update a row of $T$ corresponding to an unseen feature, $c$. From equation ??? and ??? both $\mathbf{H}_c$ and $\mathbf{g}_c$ will then be all zero \footnote{If we estimate $\mathbf{m}$ with the same data that we use to update $\mathbf{T}$, then equation ??? and ??? gives that $\mathbf{m}_c$ will equal $-\infty$, making $\phi_{nc}=\gamma_{nc}=0$ for all utterances, $n$, in the training set.}. Since it is clear that any values for $\mathbf{t}_c$ will be a valid solution, a simple way to circumvent the problem is to assume that the new values for $\mathbf{t}_c$ is also zero. This will make the gradient and Hessian for iVectors (equation ??? and ???) unaffected by features not seen in the training data. Since it is unlikely that we gain much information  from those features anyways, we will have reduced the complexity of updating iVectors and $\mathbf{T}$ by ignoring these rows, without significant cost.

There are some other situations situations where $\mathbf{H}_x$ will be singular. Unfortunate initial values for $\mathbf{T}$ could produce a highly linearly dependent matrix. E.g. if $\mathbf{T}$ is all zero then $\mathbf{H}_n$ would certainly be singular. Also with the iVector dimension, $R$, set very high, we would expect some of the dimensions to be redundant when maximizing equation ???. This would imply that the i-vectors no longer represent global relationships between utterances, but that each i-vector is individually tailored to give the best possible likelihood for the utterance. With a random initialization of $\mathbf{T}$, and i-vectors of lengths that actually compress the information in utterances, the Hessian should be nonsingular in practice. More formal conditions for nonsingularity is given in section \ref{posdef}.

This shouldn't be an issue since our aim is to compactly represent utterances. 

\subsection{Proof of symmetric Hessian}

From equations ??? and ???, $\mathbf{H}_x$ can be seen to be symmetric.
\begin{equation}\label{symproof}
\mathbf{H}_x^T = \left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)^T = 
\sum_{\forall y}a_{xy}\left(\mathbf{y}\mathbf{y}^T\right)^T =
 \sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T = \mathbf{H}_x
\end{equation}
where $a_{xy}$ is the scalar part of the Hessian's equation. This is a very important property as the construction of the Hessian is computationally expensive. The outer vector product used in the Hessian requires $\mathcal{O}(R^2)$ multiplications and is repeated $NC$ times when updating $N$ iVectors or $C$ rows of $\mathbf{T}$, making the asymptotic runtime $\mathcal{O}(NCR^2)$.  Since most of the matrix is redundant, we only have to calculate the upper or lower half of the matrix. While the asymptotic runtime remains the same, the actual running time of the algorithm should be nearly halved.

\subsection{Proof of Hessian being positive definite}
\label{posdef}

For a special class of linear systems, there exists a number of faster solvers than LUP decomposition. We will here show that $\mathbf{H}_x$ is positive definite, making $\mathcal{O}(R^2)$ algorithms applicable \cite{tewfik1994fast} to solve the linear system in equation \ref{lupec}. Given that $R$ should be significantly less than $C$, the reduction in total runtime could be negligible to the time spent to set up the equation's Hessian. Nevertheless, proving $\mathbf{H}_x$ is positive definite enables us to use less complex (and still faster) solvers like the closely related LU decomposition \cite[p. 749]{cormen}. A matrix, $\mathbf{A}$ is given to be positive definite if $\mathbf{z}^T\mathbf{Az} > 0$ for all non-zero vectors $\mathbf{z}$ \cite[p. 246]{matte3}. For $\mathbf{H}_x$ we have that

\begin{equation}
\mathbf{z}^T\mathbf{H}_x\mathbf{z} = \mathbf{z}^T\left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)\mathbf{z} 
= \sum_{\forall y}a_{xy}\mathbf{z}^T\mathbf{y}\mathbf{y}^T\mathbf{z} 
= \sum_{\forall y}a_{xy}\left(\mathbf{z}^T\mathbf{y}\right)^2 \geq 0 
\end{equation}
The inequality above is given by the fact that $a_{xy} \geq 0$ and that $\mathbf{z}^T\mathbf{y}$ is a real scalar. If $\mathbf{\tilde{y}}$ is the set of $\mathbf{y}$-vectors where $a_{xy} \neq 0$, then equality can only be reached if $\mathbf{z}$ is orthogonal to all the vectors in $\mathbf{\tilde{y}}$. This implies that the vectors in $\mathbf{\tilde{y}}$ cannot span the entire $R$-dimensional space. Each row of $\mathbf{H}_x$ will be a weighted sum of these vectors, so the span of rows in $\mathbf{H}_x$ will be contained in the subspace defined by the vectors in $\mathbf{\tilde{y}}$. Therefore $\mathbf{H}_x$ must also have a rank less than $R$ and be singular. Interestingly, a positive definite matrix will always be nonsingular CITE!!!, which means that if, and only if, the vectors in $\mathbf{\tilde{y}}$ span the whole space, then $\mathbf{H}_x$ will be nonsingular and positive definite. 

Given that $\mathbf{T}$ and iVectors only get finite values from updates, $a_xy = \max(\gamma_{nc}, \phi_{nc})$ can only be zero if $m_c=-\infty$. So if training i-vectors and rows of $\mathbf{T}$ representing features seen in the training set span $R$, the Hessian will be nonsingular for updates of $\mathbf{T}$ and i-vectors respectively. It should then be sufficient to set $R$ significantly lower than the number of unique features and utterances in the training set to keep the Hessian nonsingular.

\subsection{Span of the total variability matrix}

In order to achieve a high likelihood from equation ???, $\mathbf{T}$ needs to model the variability between utterances. T

We will here show that the achievable likelihood only depends on the column span of $\mathbf{T}$. This means that techniques like orthogonalizing $\mathbf{T}$ won't have any effect on 




\section{Gaussian backend}


