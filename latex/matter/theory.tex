\chapter{Theory}
\label{sect:Theory}

\section{Properties of the Newton Raphson updates}

Here we are going to look more closely at the Newton Raphson update steps for producing iVectors and the variability matrix. In a real-time implementation we would need to extract iVectors using these updates for live data which makes the operation time-sensitive. At the same time these updates involve linear algebra on a high-dimension space, making the operations computationally expensive. An unefficient implementation of these updates will therefore severely impact the computational requirements for training and live usage of the system. It is also critical for the total performance of the system that the iVectors convey meaningful information about an utterance. A thorough study of the Newton Raphson update process is therefore warranted. 

Due to the similarities in updating rows of $\mathbf{T}$ and iVectors we will here use a common notation when proofs apply to both updates. $\mathbf{x}$ will denote the updated vector (either a row of $\mathbf{T}$ or an iVector), $\mathbf{H}_x$ its (approximate) Hessian and $\mathbf{g}_x$ its gradient. We will use $\mathbf{y}$ to denote the vectors used to update $\mathbf{x}$ (iVectors if $\mathbf{x}$ is a row of $\mathbf{T}$ and vice versa).

To avoid issues with numeric instability, it is often desirable not to calculate the inverse of a matrix \cite[p. 743]{cormen}. We can rewrite the linear systems in equation ??? and ??? to

\begin{equation}\label{lupeq}
\mathbf{H}_x\delta\mathbf{x} = \mathbf{g}
\end{equation}
where $\delta\mathbf{x}$ equals $\mathbf{x}(NEW)-\mathbf{x}(OLD)$. Equation \ref{lupeq} can then be solved by using well known algorithms such as LUP decomposition. With a given iVector dimension $R$, the system is then solved in $\mathcal{O}(R^3)$ asymptotic time \cite[p. 754]{cormen}. 

LUP decomposition can only solve this system when $\mathbf{H}_x$ is nonsingular. That is when the rank of $\mathbf{H}_x$ is $R$, and there exists one and only one solution \cite[p. 54]{matte3}. This is not the case when we during training update a row of $T$ corresponding to an unseen feature, $c$. From equation ??? and ??? both $\mathbf{H}_c$ and $\mathbf{g}_c$ will then be all zero \footnote{If we estimate $\mathbf{m}$ with the same data that we use to update $\mathbf{T}$, then equation ??? and ??? gives that $\mathbf{m}_c$ will equal $-\infty$, making $\phi_{nc}=\gamma_{nc}=0$ for all utterances, $n$, in the training set.}. Since it is clear that any values for $\mathbf{t}_c$ will be a valid solution, a simple way to circumvent the problem is to assume that the new values for $\mathbf{t}_c$ is also zero. This will make the gradient and Hessian for iVectors (equation ??? and ???) unaffected by features not seen in the training data. Since it is unlikely that we gain much information  from those features anyways, we will have reduced the complexity of updating iVectors and $\mathbf{T}$ by ignoring these rows, without significant- or perhaps any cost.

There are some other situations situations where $\mathbf{H}_x$ will be singular. Unfortunate initial values for $\mathbf{T}$ could produce a highly linearly dependent matrix. E.g. if $\mathbf{T}$ is all zero then $\mathbf{H}_n$ would certainly be singular. Also with the iVector dimension, $R$, set very high, we would expect some of the dimensions to be redundant when maximizing equation ???. This shouldn't be an issue since our aim is to compactly represent utterances. 

Instead of finding latent information shared by utterances,  Clearly so many degrees of freedom is in violation of our goal to compactly represent utterances 


we would expect equation \ref{lupeq} to have more than one solution. The reason for this will be more formally shown when we prove that $\mathbf{H}_x$ is positive definite, but a reasonable argument for now is that with enough degrees of freedom, we would be able 

While there are other situations where $\mathbf{H}_x$ is singular
There are other situations where $\mathbf{H}$ can be singular, but they shouldn't arise MORE!!!


From equations ??? and ???, $\mathbf{H}_x$ can be seen to be symmetric.
\begin{equation}\label{symproof}
\mathbf{H}_x^T = \left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)^T = 
\sum_{\forall y}a_{xy}\left(\mathbf{y}\mathbf{y}^T\right)^T =
 \sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T = \mathbf{H}_x
\end{equation}
where $a_{xy}$ is the scalar part of the Hessian's equation. This is a very important property as the construction of the Hessian is computationally expensive. The outer vector product used in the Hessian requires $\mathcal{O}(R^2)$ multiplications and is repeated $NC$ times when updating $N$ iVectors or $C$ rows of $\mathbf{T}$, making the asymptotic runtime $\mathcal{O}(NCR^2)$.  Since most of the matrix is redundant, we only have to calculate the upper or lower half of the matrix. While the asymptotic runtime remains the same, the actual running time of the algorithm should be nearly halved.

For a special class of linear systems, there exists a number of faster solvers than LUP decomposition. We will here show that $\mathbf{H}_x$ is positive definite, making $\mathcal{O}(R^2)$ algorithms applicable \cite{tewfik1994fast} to solve the linear system in equation \ref{lupec}. Given that $R$ should be significantly less than $C$, the reduction in total runtime could be negligible to the time spent to set up the equation's Hessian. Nevertheless, proving $\mathbf{H}_x$ is positive definite enables us to use less complex (and still faster) solvers like the closely related LU decomposition \cite[p. 749]{cormen}. A matrix, $\mathbf{A}$ is given to be positive definite if $\mathbf{z}^T\mathbf{Az} > 0$ for all non-zero vectors $\mathbf{z}$ \cite[p. 246]{matte3}. For $\mathbf{H}_x$ we have that

\begin{equation}
\mathbf{z}^T\mathbf{H}_x\mathbf{z} = \mathbf{z}^T\left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)\mathbf{z} 
= \sum_{\forall y}a_{xy}\mathbf{z}^T\mathbf{y}\mathbf{y}^T\mathbf{z} 
= \sum_{\forall y}a_{xy}\left(\mathbf{z}^T\mathbf{y}\right)^2 \geq 0 
\end{equation}
The inequality above is given by the fact that $a_{xy} \geq 0$ and that $\mathbf{z}^T\mathbf{y}$ is a real scalar. If $\mathbf{\tilde{y}}$ is the set of $\mathbf{y}$-vectors where $a_{xy} \neq 0$, then equality can only be reached if $\mathbf{z}$ is orthogonal to all the vectors in $\mathbf{\tilde{y}}$. This implies that the vectors in $\mathbf{\tilde{y}}$ cannot span the entire $R$-dimensional space. Each row of $\mathbf{H}_x$ will be a weighted sum of these vectors, so the span of rows in $\mathbf{H}_x$ will be contained in the subspace defined by the vectors in $\mathbf{\tilde{y}}$. Therefore $\mathbf{H}_x$ must also have a rank less than $R$ and be singular. Interestingly, a positive definite matrix will always be nonsingular CITE!!!, which means that if, and only if, the vectors in $\mathbf{\tilde{y}}$ span the whole space, then $\mathbf{H}_x$ will be nonsingular and positive definite. 

Given that $\mathbf{T}$ and iVectors are finite, we have from equation ??? that $\phi_{nc} > 0$ whenever $m_c \neq -\infty$. This mens that $a_{xy}=\max(\gamma_{nc}, \phi_{nc})>0$ for all features, $c$, found in the utterances used to train $\mathbf{m}$ (equation ???). A sufficient condition for the Hessian to be nonsingular in practical usage should therefore be to set the iVector dimension, $R$, significantly lower than the number of unique features and utterances in the training set. Using high-precision variables for iVectors and $\mathbf{T}$ it will at least be very unlikely that thewhole $R$-dimensional space isn't spanned.




\section{Gaussian backend}


