\chapter{Proofs of properties for the Newton Raphson updates}

This chapter will prove the properties used in section \ref{sect:deeperivect}. 

%Due to the similarities in updating rows of $\mathbf{T}$ and iVectors we will here use a common notation when proofs apply to both updates. $\mathbf{x}$ will denote the updated vector (either a row of $\mathbf{T}$ or an iVector), $\mathbf{H}_x$ its (approximate) Hessian and $\mathbf{g}_x$ its gradient. We will use $\mathbf{y}$ to denote the vectors used to update $\mathbf{x}$ (iVectors if $\mathbf{x}$ is a row of $\mathbf{T}$ and vice versa). The Hessian from equation ?? and ?? is then written as

\section{Proof of symmetri}
\label{symproof}

From equation \ref{ivecthessian} we have

\begin{align*}
\mathbf{H}_n^T &= \left(\sum_{i=1}^{C}\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})\mathbf{t}_i\mathbf{t}_i^T\right)^T \\
 &= \sum_{i=1}^{C}\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})\left(\mathbf{t}_i\mathbf{t}_i^T\right)^T \\
 &= \sum_{i=1}^{C}\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})\mathbf{t}_i\mathbf{t}_i^T \\
 &= \mathbf{H}_n
\end{align*}
which concludes the proof that $\mathbf{H}_n$ is symmetric. A similar proof using equation \ref{thessian} will show that $\mathbf{H}_c$ is symmetric as well.


%\begin{equation}\label{symproof}
%\mathbf{H}_x^T = \left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)^T = 
%\sum_{\forall y}a_{xy}\left(\mathbf{y}\mathbf{y}^T\right)^T =
% \sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T = \mathbf{H}_x
%\end{equation}


\section{Conditions for positive definity and for nonsingularity}
\label{posdefproof}
We will here show the conditions for the Hessian of i-vectors and rows of $\mathbf{T}$ being positive definite and nonsingular. A nonsingular matrix, $\mathbf{A}$ is a matrix that has one and only one solution to a linear system $\mathbf{Ax}=\mathbf{b}$ \cite[p. 54]{matte3}. Also $\mathbf{A}$ is positive definite if $\mathbf{x}^T\mathbf{Ax} > 0$ for all non-zero vectors $\mathbf{x}$ \cite[p. 246]{matte3}. When we apply this to the approximation to the Hessian from equation \ref{ivecthessian} we get

\begin{align}
\mathbf{x}^T\mathbf{H}_n\mathbf{x} &= \mathbf{x}^T\left(\sum_{i=1}^C\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})\mathbf{t}_i\mathbf{t}_i^T\right)\mathbf{x} \nonumber \\
&= \sum_{i=1}^C\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})\mathbf{x}^T\mathbf{t}_i\mathbf{t}_i^T\mathbf{x} \label{posdefEq}\\
&= \sum_{i=1}^C\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})(\mathbf{x}^T\mathbf{t}_i)^2 \geq 0 \nonumber
\end{align}
The inequality above is given by the fact that $\phi_{nc}$ and $\gamma_{nc} \geq 0 \forall n, i$ and that $\mathbf{x}^T\mathbf{t}_c$ will be a real scalar. Since the update process explained in section \ref{sect:ivecttrain} only gives i-vectors and rows of $\mathbf{T}$ finite values $\phi_{nc}$ from equation \ref{phieq} will only equal 0 when $m_c = -\infty$. From equation \ref{mest}, this will only happen if the feature, $c$, is not seen in the set used to calculate $m_c$. We now let $\mathbf{\tilde{t}}_n$ denote the set of rows from $\mathbf{T}$ that correspond to features seen in the training set or the current vector $n$, that is

\begin{equation*}
\mathbf{\tilde{t}}_n = \{\mathbf{t}_c | m_c \neq -\infty \cup \gamma_{nc} \neq 0\}
\end{equation*}
Equality in equation \ref{posdefEq} can only be reached if $\mathbf{x}$ is orthogonal to all the vectors in $\mathbf{\tilde{t}}_n$. If $R$ is the dimension of rows of $\mathbf{T}$ (and iVectors), then $\mathbf{x}$ can only be orthogonal to the vectors in $\mathbf{\tilde{t}}_n$ if these vectors span a true subset of the $R$-dimensional space. Each row of $\mathbf{H}_n$ will be a weighted sum of the vectors in $\mathbf{\tilde{t}}_n$, so the row span of $\mathbf{H}_n$ will be a subspace of $\mathbf{\tilde{t}}_n$'s span. If there is an orthogonal vector $\mathbf{x}$, then $\mathbf{H}_n$ will have rank less than $R$ and be singular \cite[p. 54]{matte3}. Since it is given that a positive definite matrix also is nonsingular \cite[760]{cormen},  we have that $\mathbf{H}_n$ will be nonsingular and positive definite if and only if the vectors in $\mathbf{\tilde{t}}_n$ span the entire $R$-dimensional space.

%\begin{equation}
%\mathbf{z}^T\mathbf{H}_x\mathbf{z} = \mathbf{z}^T\left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)\mathbf{z} 
%= \sum_{\forall y}a_{xy}\mathbf{z}^T\mathbf{y}\mathbf{y}^T\mathbf{z} 
%= \sum_{\forall y}a_{xy}\left(\mathbf{z}^T\mathbf{y}\right)^2 \geq 0 
%\end{equation}

With similar calculations as in equation \ref{posdefEq} we can show when $\mathbf{H}_c$ is nonsingular and positive definite. As with $\mathbf{\tilde{t}}_n$ we let $\mathbf{\tilde{w}}_c$ denote the set of iVectors that contribute (isn't multiplied with zero in equation \ref{thessian}) to $\mathbf{H}_c$. Given that we use the same set to train $\mathbf{m}$ and $\mathbf{T}$, $\gamma_{nc}$ can only be zero if $m_c$ is $-\infty$. So $\mathbf{\tilde{w}}_c$ is either all iVectors from the training set, or the empty set if $c$ is a feature that is not seen in the training set. For features found in the training set, this means that $\mathbf{H}_c$ will be nonsingular and positive definite if and only if all the i-vectors from the training set span all $R$ dimensions.

\section{The importance of the total variability matrix's column span}
\label{orthproof}

In this section we will see the importance the column span of $\mathbf{T}$ has on the likelihood achieved during the iterative updates given in section \ref{sect:ivecttrain}. Given two different matrices, $\mathbf{T}$ and $\mathbf{\hat{T}}$, with the same column span, there will for any vector $\mathbf{w}_n$ exist a vector $\mathbf{\hat{w}}_n$ where 

\begin{equation}
\label{colpropEq}
\mathbf{Tw}_n = \mathbf{\hat{T}\hat{w}}_n
\end{equation}
REF ???.
