\chapter{Theory}
\label{sect:Theory}

\section{More on the iterative update process}

Here we are going to look more closely at the Newton Raphson update steps for producing i-vectors and the variability matrix. In a real-time implementation we would need to extract i-vectors using these updates for live data which makes the operation time-sensitive. At the same time these updates involve linear algebra on a high-dimension space, making the operations computationally expensive. An unefficient implementation of these updates will therefore severely impact the computational requirements for training and live usage of the system. It is also critical for the total performance of the system that the i-vectors convey meaningful information about an utterance. A thorough study of the Newton Raphson update process is therefore warranted. 

\subsection{Solving the Newton Raphson systems}

To avoid issues with numeric instability, it is often desirable not to calculate the inverse of a matrix \cite[p. 743]{cormen}. We can rewrite the linear systems in equation ??? and ??? to

\begin{equation}\label{lupeqN}
\mathbf{H}_n(\text{old}) \delta\mathbf{w}_n = \mathbf{g}_n
\end{equation}
and
\begin{equation}\label{lupeqC}
\mathbf{H}_c(\text{old}) \delta \mathbf{t}_c = \mathbf{g}_c(\text{old})
\end{equation}
respectively where $\delta$ means the difference between the new and old vectors. It is beneficial to ensure that these equations have one, and just one, solution. More than one solution would indicate that some of the dimensions in the rows of $\mathbf{T}$ or i-vector is redundant, making us solve a more complicated problem than strictly needed. The requirements on $\mathbf{T}$ and i-vectors to guarantee just one solution, is shown in appendix \ref{posdefproof}. As long as our goal is to find global relationships between utterances, and not overfit i-vectors to each utterance (by letting the i-vector dimension approach the number of training utterances or unique features), these requirements should be met. One exception is when we update rows of $\mathbf{T}$ that correspond to features not seen in the training set. Since it is unlikely that we gain much information from such features anyways, we can assume that those rows are always all zero. The rows can then be ignored during i-vector updates without much, if any, loss in performance.

In appendix \ref{posdefproof} we also show that the Hessian in equation \ref{lupeqN} and \ref{lupeqC} are positive definite. This enables us to use simple algorithms like LU decomposition to solve the systems \cite[p. 749]{cormen}. With an i-vector dimension of $R$, LU decomposition will solve the systems in $\mathcal{O}(R^3)$ asymptotic time CORMEN REF. While there are faster solvers for positive definite systems like the $\mathcal{O}(R^2)$ solver in \cite{tewfik1994fast}, $R$ should be of a size that probably doesn't necessitate excessive optimization.

In equation ??? $\mathbf{H}_n$ is found by calculating the outer product of rows of $\mathbf{T}$ $C$ times, making the asymptotic runtime $\Omega(CR^2)$. Similary for $\mathbf{H}_c$ the asymptotic runtime of equation ??? is  $\Omega(NR^2)$. Since $R$ should be significantly less than $N$ and $C$ to ensure that the linear systems only have one solution, calculating the Hessian will be more computationally demanding than solving the systems. In appendix \ref{symproof} we show that the Hessian is symmetric, which enables us to only calculate the upper (or lower) half of the matrices resulting from the outer products. While the asymptotic runtime remains the same, the actual running time of should be nearly halved.

\subsection{Achieving higher likelihoods}

In order to achieve a high likelihood from equation ???, $\mathbf{T}$ needs to model the variability between utterances. T

We will here show that the achievable likelihood only depends on the column span of $\mathbf{T}$. This means that techniques like orthogonalizing $\mathbf{T}$ won't have any effect on 




\section{Gaussian backend}


