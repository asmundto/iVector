\chapter{Proofs of properties for the Newton Raphson updates}

This chapter will prove the properties used in section \ref{sect:deeperivect}. 

%Due to the similarities in updating rows of $\mathbf{T}$ and iVectors we will here use a common notation when proofs apply to both updates. $\mathbf{x}$ will denote the updated vector (either a row of $\mathbf{T}$ or an iVector), $\mathbf{H}_x$ its (approximate) Hessian and $\mathbf{g}_x$ its gradient. We will use $\mathbf{y}$ to denote the vectors used to update $\mathbf{x}$ (iVectors if $\mathbf{x}$ is a row of $\mathbf{T}$ and vice versa). The Hessian from equation ?? and ?? is then written as

\section{Proof of symmetri}
\label{symproof}

From equation \ref{ivecthessian} we have

\begin{align*}
\mathbf{H}_n^T &= \left(\sum_{i=1}^{C}\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})\mathbf{t}_i\mathbf{t}_i^T\right)^T \\
 &= \sum_{i=1}^{C}\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})\left(\mathbf{t}_i\mathbf{t}_i^T\right)^T \\
 &= \sum_{i=1}^{C}\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})\mathbf{t}_i\mathbf{t}_i^T \\
 &= \mathbf{H}_n
\end{align*}
which concludes the proof that $\mathbf{H}_n$ is symmetric. A similar proof using equation \ref{thessian} will show that $\mathbf{H}_c$ is symmetric as well.


%\begin{equation}\label{symproof}
%\mathbf{H}_x^T = \left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)^T = 
%\sum_{\forall y}a_{xy}\left(\mathbf{y}\mathbf{y}^T\right)^T =
% \sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T = \mathbf{H}_x
%\end{equation}


\section{Conditions for positive definity and for nonsingularity}
\label{posdefproof}
We will here show the conditions for the Hessian of iVectors and rows of $\mathbf{T}$ being positive definite and nonsingular. A nonsingular matrix, $\mathbf{A}$ is a matrix that has one and only one solution to a linear system $\mathbf{Ax}=\mathbf{b}$ \cite[p. 54]{matte3}. Also $\mathbf{A}$ is positive definite if $\mathbf{x}^T\mathbf{Ax} > 0$ for all non-zero vectors $\mathbf{x}$ \cite[p. 246]{matte3}. When we apply this to the approximation to the Hessian from equation \ref{ivecthessian} we get

\begin{align}
\mathbf{x}^T\mathbf{H}_n\mathbf{x} &= \mathbf{x}^T\left(\sum_{i=1}^C\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})\mathbf{t}_i\mathbf{t}_i^T\right)\mathbf{x} \nonumber \\
&= \sum_{i=1}^C\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})\mathbf{x}^T\mathbf{t}_i\mathbf{t}_i^T\mathbf{x} \label{posdefEq}\\
&= \sum_{i=1}^C\max(\gamma_{ni}, \phi_{ni}\sum_{j=1}^{C}\gamma_{nj})(\mathbf{x}^T\mathbf{t}_i)^2 \geq 0 \nonumber
\end{align}
The inequality above is given by the fact that $\phi_{nc}$ and $\gamma_{nc} \geq 0 \forall n, i$ and that $\mathbf{x}^T\mathbf{t}_c$ will be a real scalar. Since the update process explained in section \ref{sect:ivecttrain} only gives iVectors and rows of $\mathbf{T}$ finite values $\phi_{nc}$ from equation \ref{phieq} will only equal 0 when $m_c = -\infty$. From equation \ref{mest}, this will only happen if the feature, $c$, is not seen in the set used to calculate $m_c$. We now let $\mathbf{\tilde{t}}_n$ denote the set of rows from $\mathbf{T}$ that correspond to features seen in the training set or the current vector $n$, that is

\begin{equation*}
\mathbf{\tilde{t}}_n = \{\mathbf{t}_c | m_c \neq -\infty \cup \gamma_{nc} \neq 0\}
\end{equation*}
Equality in equation \ref{posdefEq} can only be reached if $\mathbf{x}$ is orthogonal to all the vectors in $\mathbf{\tilde{t}}_n$. If $R$ is the dimension of rows of $\mathbf{T}$ (and iVectors), then $\mathbf{x}$ can only be orthogonal to the vectors in $\mathbf{\tilde{t}}_n$ if these vectors span a true subset of the $R$-dimensional space. Each row of $\mathbf{H}_n$ will be a weighted sum of the vectors in $\mathbf{\tilde{t}}_n$, so the row span of $\mathbf{H}_n$ will be a subspace of $\mathbf{\tilde{t}}_n$'s span. If there is an orthogonal vector $\mathbf{x}$, then $\mathbf{H}_n$ will have rank less than $R$ and be singular \cite[p. 54]{matte3}. Since it is given that a positive definite matrix also is nonsingular \cite[p. 760]{cormen},  we have that $\mathbf{H}_n$ will be nonsingular and positive definite if and only if the vectors in $\mathbf{\tilde{t}}_n$ span the entire $R$-dimensional space.

%\begin{equation}
%\mathbf{z}^T\mathbf{H}_x\mathbf{z} = \mathbf{z}^T\left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)\mathbf{z} 
%= \sum_{\forall y}a_{xy}\mathbf{z}^T\mathbf{y}\mathbf{y}^T\mathbf{z} 
%= \sum_{\forall y}a_{xy}\left(\mathbf{z}^T\mathbf{y}\right)^2 \geq 0 
%\end{equation}

With similar calculations as in equation \ref{posdefEq} we can show when $\mathbf{H}_c$ is nonsingular and positive definite. As with $\mathbf{\tilde{t}}_n$ we let $\mathbf{\tilde{w}}_c$ denote the set of iVectors that contribute (isn't multiplied with zero in equation \ref{thessian}) to $\mathbf{H}_c$. Given that we use the same set to train $\mathbf{m}$ and $\mathbf{T}$, $\gamma_{nc}$ can only be zero if $m_c$ is $-\infty$. So $\mathbf{\tilde{w}}_c$ is either all iVectors from the training set, or the empty set if $c$ is a feature that is not seen in the training set. For features found in the training set, this means that $\mathbf{H}_c$ will be nonsingular and positive definite if and only if all the i-vectors from the training set span all $R$ dimensions.

\section{Likelihoods dependence on total variability matrix's column span}
\label{sect:orthproof}

Given two total variability matrices, $\mathbf{T}$ and $\mathbf{\hat{T}}$, with the same column span, there will for any iVector $\mathbf{w}_n$ exist an iVector $\mathbf{\hat{w}}_n$ where 

\begin{equation}
\label{colpropEq}
\mathbf{Tw}_n = \mathbf{\hat{T}\hat{w}}_n.
\end{equation}
This is because both sides of the equation will be a linear combinations of the column vectors of $\mathbf{T}$ or $\mathbf{\hat{T}}$ that share the same subspace. 

We'll here show that if two iVectors satisfy equation \ref{colpropEq} using some total variability matrices with common column span, then the iVectors we get after a Newton Rapshon update will also satisfy equation \ref{colpropEq}. From equation \ref{phieq}, this will also imply that $\phi_{nc}$ and thereby the likelihood when using either of the total variability matrices with the corresponding iVectors will be equal before and after an update. By inserting equation \ref{ivectgrad} and \ref{ivecthessian}, the equations for the gradient and Hessian respectively, into equation \ref{lupeqN}, a Newton Rapshon update will then be
\begin{align}
\sum_{c=1}^C b_{nc} \mathbf{t}_c \mathbf{t}_c^T(\mathbf{w}_n(\text{new})-\mathbf{w}_n(\text{old})) = \sum_{c=1}^C a_{nc} \mathbf{t}_c \nonumber \\
\sum_{c=1}^C \left(b_{nc} \mathbf{t}_c \mathbf{t}_c^T(\mathbf{w}_n(\text{new})-\mathbf{w}_n(\text{old})) - a_{nc} \mathbf{t}_c\right) = 0 \label{longlupeq}
\end{align}
where the scalars
\begin{equation*}
a_{nc} = \left(\gamma_{nc}-\phi_{nc}(\text{old})\sum_{i=1}^C \gamma_{ni} \right)
\end{equation*}
and
\begin{equation*}
b_{nc} = \max\left(\gamma_{nc}, \phi_{nc}(\text{old})\sum_{i=1}^C \gamma_{ni} \right).
\end{equation*}
If this equality holds using $\mathbf{t}_c$, $\mathbf{w}_n(\text{new})$ and $\mathbf{w}_n(\text{old})$, then we only need to check that it holds when we instead use $\mathbf{\hat{t}}_c$ and the iVectors $\mathbf{\hat{w}}_n(\text{old})$ and $\mathbf{\hat{w}}_n(\text{new})$ satisfying equation \ref{colpropEq} that we know exists. By using the condition in equation \ref{colpropEq} in equation \ref{longlupeq} we have

\begin{align}
\sum_{c=1}^C & \left( b_{nc} \mathbf{t}_c \mathbf{t}_c^T(\mathbf{w}_n(\text{new})-\mathbf{w}_n(\text{old})) - a_{nc} \mathbf{t}_c \right) =  \nonumber \\
& \sum_{c=1}^C \left( b_{nc} \mathbf{t}_c \mathbf{\hat{t}}_c^T(\mathbf{\hat{w}}_n(\text{new})-\mathbf{\hat{w}}_n(\text{old})) - a_{nc} \mathbf{t}_c \right) = \nonumber \\
& \sum_{c=1}^C k_{nc} \mathbf{t}_c = 0 \label{colspanproof}
\end{align}
where
\begin{equation*}
k_{nc} = \left(b_{nc}\mathbf{\hat{t}}_c^T(\mathbf{\hat{w}}_n(\text{new})-\mathbf{\hat{w}}_n(\text{old})) - a_{nc}\right).
\end{equation*}
Since $\phi_{nc}(\text{old})$ is equal for total variability matrices and iVectors satisfying equation \ref{colpropEq}, $a_{nc}$, $b_{nc}$ and thereby $k_{nc}$ will also be unaffected by the change of matrix and iVectors. We can conclude the proof if $\mathbf{t}_c$ can be interchanged with $\mathbf{\hat{t}}_c$ in equation \ref{colspanproof} which can also be written as
\begin{equation}
\label{matcolspanproof}
\mathbf{T}^T \mathbf{k}_n = 0,
\end{equation}
where the $i$-th element in the vector $\mathbf{k}_n$ is $k_{ni}$. Since the matrices span the same column space, $\mathbf{k}_n$ will either be orthogonal to neither or both the rows of  $\mathbf{T}^T$ and $\mathbf{\hat{T}}^T$. Assuming that the system is nonsingular, then equation \ref{matcolspanproof} will have a solution, and when equation \ref{colpropEq} is satisfied for the old iVectors the nonsingularity will imply that the only values for possible values for the new iVectors will also satisfy that constraint.
