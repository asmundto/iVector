\chapter{Theory}
\label{sect:Theory}

\section{Properties of the Newton Raphson updates}

Here we are going to look more closely at the Newton Raphson update steps for producing iVectors and the variability matrix. In a real-time implementation we would need to extract iVectors using these updates for live data which makes the operation time-sensitive. At the same time these updates involve linear algebra on a high-dimension space, making the operations computationally expensive. An unefficient implementation of these updates will therefore severely impact the computational requirements for training and live usage of the system. It is also critical for the total performance of the system that the iVectors convey meaningful information about an utterance. A thorough study of the Newton Raphson update process is therefore warranted. 

Due to the similarities in updating rows of $\mathbf{T}$ and iVectors we will here use a common notation when proofs apply to both updates. $\mathbf{x}$ will denote the updated vector (either a row of $\mathbf{T}$ or an iVector), $\mathbf{H}_x$ its jacobian and $\mathbf{g}_x$ its gradient. We will use $\mathbf{y}$ to denote the vectors used to update $\mathbf{x}$ (iVectors if $\mathbf{x}$ is a row of $\mathbf{T}$ and vice versa).

To avoid issues with numeric instability, it is often desirable not to calculate the inverse of a matrix CORMEN743. We can rewrite the linear systems in equation ??? and ??? to

\begin{equation}\label{lupeq}
\mathbf{H}\delta\mathbf{x} = \mathbf{g}
\end{equation}
where $\delta\mathbf{x}$ equals $\mathbf{x}(NEW)-\mathbf{x}(OLD)$. Equation \ref{lupeq} can then be solved by using well known algorithms such as LUP decomposition. With a given iVector dimension $R$, the system is then solved in $\mathcal{O}(R^3)$ asymptotic time CORMEN754. 

LUP decomposition can only solve this system when $\mathbf{H}$ is singular. That is when the rank of $\mathbf{H}$ is $R$, and there exists one and only one solution ELEMLINALGEBRA. This is not the case when we during training update a row of $T$ corresponding to an unseen feature, $c$. From equation ??? and ??? both $\mathbf{H}_c$ and $\mathbf{g}_c$ will then be all zero \footnote{If we estimate $\mathbf{m}$ with the same data that we use to update $\mathbf{T}$, then equation ??? and ??? gives that $\mathbf{m}_c$ will equal $-\infty$, making $\phi_{nc}=\gamma_{nc}=0$ for all utterances, $n$, in the training set.}. Since it is clear that any values for $\mathbf{t}_c$ will be a valid solution, a simple way to circumvent the problem is to assume that the new values for $\mathbf{t}_c$ is also zero. This will make the gradient and jacobian for iVectors (equation ??? and ???) unaffected by features not seen in the training data. Since it is unlikely that we gain much information  from those features anyways, we will have reduced the complexity of updating iVectors and $\mathbf{T}$ by ignoring these rows, without significant- or perhaps any cost.

There are other situations where $\mathbf{H}$ can be singular, but they shouldn't arise MORE!!!


From equations ??? and ???, $\mathbf{H}_x$ can be seen to be symmetric.
\begin{equation}\label{symproof}
\mathbf{H}_x^T = \left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)^T = 
\sum_{\forall y}a_{xy}\left(\mathbf{y}\mathbf{y}^T\right)^T =
 \sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T = \mathbf{H}_x
\end{equation}
where $a_{xy}$ is the scalar part of the jacobians equation. This is a very important property as the construction of the jacobian is computationally expensive. Naively, the outer vector product used in the jacobian requires $R^2$ multiplications and is repeated $NC$ times when updating $N$ iVectors or $C$ rows of $\mathbf{T}$, making the asymptotic runtime $\mathcal{O}(NCR^2)$.  Since most of the matrix is redundant, we only have to calculate the upper or lower half of the matrix. While the asymptotic runtime remains the same, the actual running time of the algorithm should be nearly halved.

For a special class of linear systems, there exists a number of faster solvers than LUP decomposition. We will here show that $\mathbf{H}_x$ is positive definite, making $\mathcal{O}(R^2)$ algorithms applicable \url{http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=277849\&userType=inst} to solve the linear system in equation \ref{lupec}. Given that $R$ should be significantly less than $C$, the reduction in total runtime could be negligible to the time spent to set up the equation's jacobian. Nevertheless, proving $\mathbf{H}_x$ is positive definite enables us to use less complex (and still faster) solvers like the closely related LU decomposition CORMEN750ISH. A matrix, $\mathbf{A}$ is given to be positive definite if $\mathbf{z}^T\mathbf{Az} > 0$ for all non-zero vectors $\mathbf{z}$ CORMEN. For $\mathbf{H}_x$ we have that

\begin{equation}
\mathbf{z}^T\mathbf{H}_x\mathbf{z} = \mathbf{z}^T\left(\sum_{\forall y}a_{xy}\mathbf{y}\mathbf{y}^T\right)\mathbf{z} 
= \sum_{\forall y}a_{xy}\mathbf{z}^T\mathbf{y}\mathbf{y}^T\mathbf{z} 
= \sum_{\forall y}a_{xy}\left(\mathbf{z}^T\mathbf{y}\right)^2 \geq 0 
\end{equation}
The inequality above is given by the fact that $a_{xy} \geq 0$ and that $\mathbf{z}^T\mathbf{y}$ is a real scalar. If $\mathbf{\tilde{y}}$ is the set of $\mathbf{y}$-vectors where $a_{xy} \neq 0$, then equality can only be reached if $\mathbf{z}$ is orthogonal to all the vectors in $\mathbf{\tilde{y}}$. This implies that the vectors in $\mathbf{\tilde{y}}$ cannot span the entire $R$-dimensional space. Each row of $\mathbf{H}_x$ will be a weighted sum of these vectors, so a row of $\mathbf{H}_x$ will be contained in the subspace defined by $\mathbf{\tilde{y}}$. Therefore $\mathbf{H}_x$ must have a rank less than $R$ and be singular. Since $\mathbf{H}_x$ should be nonsingular in practical usage, we can assume that it also will be positive definite.  

