\chapter{iVector Subspace Modeling}

iVectors are one example of subspace modelling approaches that can be used to reduce the dimensionality of the data before training and applying classifiers to recognize the language used in an utterance. The dimensionality reduction should make training of the classifiers less computational expensive, which could enable us to train the system with more data. The goal of the reduction is to separate trends in the data that are common to all languages from the information that is unique between utterances. If this separation is properly performed, 

\section{Background}

The idea for iVectors first came from the joint factor analysis (JFA) model used in speaker verification \cite{dehak2011front}. In JFA continuous speech features are generated from a multivariate Gaussian model
\begin{equation}
\label{jfa}
\mathbf{M}=\mathbf{m}+\mathbf{Vy}+\mathbf{Ux}+\mathbf{Dz}
\end{equation}
where $\mathbf{y}$, $\mathbf{x}$ and $\mathbf{z}$ are low dimensional normal distributed vectors with zero mean and unit diagonal covariance \cite{dehak2011front}, while $\mathbf{m}$ is the mean distribution vector. By careful training the column span of $\mathbf{U}$ should model the possible effects of channel variability, while $\mathbf{V}$ and $\mathbf{D}$ should model variations in speakers. By having $\mathbf{m}$, $\mathbf{V}$ and $\mathbf{D}$ is fixed for all utterances, the speaker dependent information in the utterance would be isolated in the low-dimensional vectors $\mathbf{y}$ and $\mathbf{z}$ which then can be used to recognize speakers. In \cite{dehak2011front} it was proposed to use a total variability matrix to model the distribution of $\mathbf{M}$ by
\begin{equation}
\label{ffa}
\mathbf{M}=\mathbf{m}+\mathbf{Tw}.
\end{equation}
The low-dimensional vector $\mathbf{w}$, called iVector, would now be affected by channel characteristics. Recognition of the speaker could still be performed by first filtering out channel-dependent information in $\mathbf{w}$. This framework was then adapted in \cite{srivector} to model discrete features by assuming a multinomial distribution where the probability of feature $c$ given utterance $n$ would be 
\begin{equation}
\label{phieq}
\phi_{nc}=\frac{\exp(m_c+\mathbf{t}_c \cdot \mathbf{w}_n)}{\sum_{i=1}^C \exp(m_i+\mathbf{t}_i \cdot \mathbf{w}_n)}
\end{equaiton}
where $C$ is the total number of discrete features, $\mathbf{t}_i$ is the $i$-th row of the total variability matrix $\mathbf{T}$, and $m_i$ the $i$-th element of the $\mathbf{m}$-vector. This model was then utilized for language recognition in \cite{lrivector}.

\section{Interpretation of iVector Model}

 The iVector can be seen as a low-dimensional set of parameters that govern the probability distribution for features in any utterance. The columns of $\mathbf{T}$ should then span the subspace of likely probability distributions for features \cite{srivector}. 

\section{Model Training}

\section{Model Recognition}

\section{More on the iterative update process}

Here we are going to look more closely at the Newton Raphson update steps for producing i-vectors and the variability matrix. In a real-time implementation we would need to extract i-vectors using these updates for live data which makes the operation time-sensitive. At the same time these updates involve linear algebra on a high-dimension space, making the operations computationally expensive. An unefficient implementation of these updates will therefore severely impact the computational requirements for training and live usage of the system. It is also critical for the total performance of the system that the i-vectors convey meaningful information about an utterance. A thorough study of the Newton Raphson update process is therefore warranted. 

\subsection{Solving the Newton Raphson systems}

To avoid issues with numeric instability, it is often desirable not to calculate the inverse of a matrix \cite[p. 743]{cormen}. We can rewrite the linear systems in equation ??? and ??? to

\begin{equation}\label{lupeqN}
\mathbf{H}_n(\text{old}) \delta\mathbf{w}_n = \mathbf{g}_n
\end{equation}
and
\begin{equation}\label{lupeqC}
\mathbf{H}_c(\text{old}) \delta \mathbf{t}_c = \mathbf{g}_c(\text{old})
\end{equation}
respectively where $\delta$ means the difference between the new and old vectors. It is beneficial to ensure that these equations have one, and just one, solution. More than one solution would indicate that some of the dimensions in the rows of $\mathbf{T}$ or i-vector is redundant, making us solve a more complicated problem than strictly needed. The requirements on $\mathbf{T}$ and i-vectors to guarantee just one solution, is shown in appendix \ref{posdefproof}. As long as our goal is to find global relationships between utterances, and not overfit i-vectors to each utterance (by letting the i-vector dimension approach the number of training utterances or unique features), these requirements should be met. One exception is when we update rows of $\mathbf{T}$ that correspond to features not seen in the training set. Since it is unlikely that we gain much information from such features anyways, we can assume that those rows are always all zero. The rows can then be ignored during i-vector updates without much, if any, loss in performance.

In appendix \ref{posdefproof} we also show that the Hessian in equation \ref{lupeqN} and \ref{lupeqC} are positive definite. This enables us to use simple algorithms like LU decomposition to solve the systems \cite[p. 749]{cormen}. With an i-vector dimension of $R$, LU decomposition will solve the systems in $\mathcal{O}(R^3)$ asymptotic time CORMEN REF. While there are faster solvers for positive definite systems like the $\mathcal{O}(R^2)$ solver in \cite{tewfik1994fast}, $R$ should be of a size that probably doesn't necessitate excessive optimization.

In equation ??? $\mathbf{H}_n$ is found by calculating the outer product of rows of $\mathbf{T}$ $C$ times, making the asymptotic runtime $\Omega(CR^2)$. Similary for $\mathbf{H}_c$ the asymptotic runtime of equation ??? is  $\Omega(NR^2)$. Since $R$ should be significantly less than $N$ and $C$ to ensure that the linear systems only have one solution, calculating the Hessian will be more computationally demanding than solving the resulting linear systems. In appendix \ref{symproof} we show that the Hessian is symmetric, which enables us to only calculate the upper (or lower) half of the Hessians. While the asymptotic runtime remains the same, the actual runtime of should be nearly halved. 

\subsection{Achieving higher performance}

In an iterative algorithm it is important to ensure that each iteration brings you closer to the solution of the problem. The Newton Raphson method is oblivious to high-order derivatives, and we only use an approximation to the Hessian, so an increase in likelihood is not guaranteed from updates using equation \ref{lupeqN} and \ref{lupeqC}. In \cite{kockmann2010prosodic}, Kockmann et. al. would halve the update step until the likelihood from equation ??? increased when updating either row of $\mathbf{T}$ or i-vectors. If a higher likelihood wasn't achieved after some attempts, the old vector would be used.  

A common problem in svm classification is that the feature vector is dominated by some high-variance feature \cite{wan2005speaker}. In our subspace modeling, a high-variance in the counts of some trigrams might be a cause of concern as well. A good model of such features would likely be crucial to maximize equation ???. Not only because the number of occurrences will vary greatly between utterances, but also since a high variance implies that the feature occurs very frequently in at least some utterances. Many dimensions of the i-vector may then be spent on accurately controlling the multinomial distributions probability of producing those features. It is not clear if we gain much information from this precise fitting, rather than having coarser knowledge about the exact frequency of those features, and having more degrees of freedom in the i-vector to model other features. In \cite{soufifar2011ivector} the i-vector system's performance increased when the square root of elements in the document vector, $\gamma_n$, was used instead. By taking the square root, frequent features will be a smaller part of the total feature count, and the variance will be decreased. This should spread the importance of modeling each feature more evenly across the whole document vector.



In order to achieve a high likelihood from equation ???, $\mathbf{T}$ needs to model the variability between utterances. T

We will here show that the achievable likelihood only depends on the column span of $\mathbf{T}$. This means that techniques like orthogonalizing $\mathbf{T}$ won't have any effect on 